---
title: "Exploratory Plots"
output: github_document
---

```{r setup, include=FALSE, message=FALSE}
library(targets)
library(knitr)
library(sf)
library(showtext)
library(tigris)
library(tidyverse)
library(GGally)
library(RColorBrewer)

opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
opts_chunk$set(echo = FALSE, fig.width = 10, fig.height = 7)

font_add_google("Lato")
showtext_auto()

custom_theme <- function(...){
  theme_bw() + 
    theme(
      text = element_text(family = "Lato", size = 14),
      ...
    )
}

theme_set(custom_theme())
```

```{r}
# During interactive use, this chunk has to be loaded separately from the setup chunk.
# `targets` looks for a `_targets/` directory but cannot find one
# if the working directory is pointed at the `docs/` directory already, as is the
# case with Rmarkdown documents by default.

# The line starting with `opts_knit$set` above sets the correct working directory,
# but it only applies for chunks after the setup chunk, hence the separation of 
# the data load and the rest of the setup

# This is quite annoying workflow-wise, as the setup chunk is loaded automatically
# but this chunk isn't
mpeds_raw <- tar_read(canonical_events)
mpeds_sf <- tar_read(integrated) 
mpeds <- mpeds_sf |> st_drop_geometry()
```

# Basic counts

```{r, basic_counts}
mpeds_raw_ids <- length(unique(mpeds_raw$canonical_id))
mpeds_ids <- length(unique(mpeds$canonical_id))
n_locations <- length(unique(mpeds$location))
n_fips <- length(unique(mpeds$fips))
n_universities <- length(unique(mpeds$university))

missing_unis <- mpeds$university_locations |>
  bind_rows() |>
  filter(is.na(lat), !is.na(location))
n_missing_uni_rows <- nrow(missing_unis)
n_missing_unis <- missing_unis$location |>
  unique() |> 
  length()
```

The initial import of the MPEDS db found `r mpeds_raw_ids` 
unique canonical events, and after all cleaning steps we
still have `r mpeds_ids` canonical events.

However, there's still an issue regarding duplicate matches
in IPEDS we can detect (there are likely also incorrect matches that we can't
detect programmatically right now); there are lots of schools called
"Columbia College" (or another common name) inside IPEDS, so any schools with that name
in MPEDS will be assigned multiple schools. The MPEDS-IPEDS join
is crucial because we also use IPEDS to join county FIPS identifiers,
and thus no further joins will be accurate unless the MPEDS-IPEDS
join is accurate. As of Jan 30, 2023, we are in the middle of repairing
this join. 

Of those events, there were `r n_locations` unique locations, 
`r n_fips` unique counties, and `r n_universities` unique universities.
Surprisingly, all of the locations that were not universities 
found geocoding matches, and hand-checking the most common ones indicates that
there isn't a strong pattern of missing value substitution, e.g. Google isn't
sending the majority of results  to the centroid of America or to `(-1, -1)` or 
anything weird like that. Universities had a harder time, with `r n_missing_unis`
universities and `r n_missing_uni_rows` rows (canonical events) not returning
lon/lat coords for universities. 

That comes out to ~5% of universities not having coordinates, and
~2.5% of canonical events not having universities with coordinates.

The top universities by appearances:

```{r university_counts}

university_counts <- mpeds |> 
  group_by(university) |> 
  count() |> 
  ungroup() |> 
  drop_na() |>
  slice_max(order_by = n, n = 15)

kable(university_counts)

```
And the top locations: 


```{r location_counts}
location_counts <- mpeds |> 
  group_by(location) |> 
  count() |> 
  ungroup() |> 
  drop_na() |> 
  slice_max(order_by = n, n = 15)

kable(location_counts)
```

Top states: 

```{r state_counts}

state_fips <- fips_codes |> 
  select(state_code, state_name) |> 
  distinct()

state_counts <- mpeds |> 
  mutate(state_code = str_sub(fips, 1, 2)) |> 
  group_by(state_code) |> 
  count() |> 
  ungroup() |> 
  drop_na() |> 
  slice_max(order_by = n, n = 15) |> 
  left_join(state_fips, by = "state_code") |> 
  select(-state_code)
  
kable(state_counts)

```

And finally the top counties:

```{r county_counts}

county_fips <- fips_codes |> 
  mutate(fips = paste0(state_code, county_code),
         county_name = paste0(county, ", ", state_name)) |> 
  select(fips, county_name)

county_counts <- mpeds |> 
  group_by(fips) |> 
  count() |> 
  ungroup() |> 
  drop_na() |> 
  slice_max(order_by = n, n = 15) |> 
  left_join(county_fips, by = "fips") |> 
  select(-fips)
  
kable(county_counts)

```

These glimpses seem mostly in line with what we should expect, with a strong
caveat that the Missouri protests are not making a leading appearance in the 
counts by location, but there do seem to be a fair number in Missouri when we
take a look by state. What's going on there?

Associated semantic locations with canonical events assigned the Missouri FIPS 
code, and the Boone county FIPS code:

```{r missouri_locations}
mpeds |> 
  filter(state == "29") |> 
  count(location) |> 
  kable()

mpeds |>
  filter(fips == "29019") |>
  count(location) |> 
  kable()

```

Ah, that's not good. It seems there are non-MO locations being recognized as happening in Missouri.
See Google Doc for details

```{r police_counts}
# God that was hard
glimpse_counts <- function(colname){
  mpeds |>
    mutate("{{ colname }}" := map({{ colname }}, \(item){
      if(is.null(item)){
        return(NA)
      }
      return(item)
      })) |>
    select({{colname}}) |> 
    unnest(cols = {{colname}}) |> 
    group_by({{ colname }}) |> 
    count({{ colname }}) |> 
    arrange(desc(n)) |> 
    kable()
}

glimpse_counts(police_presence_and_size)
glimpse_counts(police_activities)
glimpse_counts(type_of_police)
```

```{r university_responses_counts}
glimpse_counts(university_action_on_issue)
glimpse_counts(university_discourse_on_issue)
glimpse_counts(university_reactions_to_protest)
```


# Counts over time

```{r basic_counts_over_time}

mpeds |> 
  mutate(date = floor_date(start_date, "month")) |> 
  group_by(date) |> 
  count() |> 
  drop_na() |> 
  ggplot(aes(x = date, y = n)) + 
  geom_line() +
  scale_x_date(breaks = "4 months",
               date_labels =  "%b",
               sec.axis = dup_axis(
                 breaks = seq(as.Date("2012-01-01"), as.Date("2018-01-01"), by = "year"),
                 labels = scales::label_date("%Y"),
               )
               ) + 
  labs(
    title = "Basic counts of canonical events over time",
    x = "Date (floored to months)",
    y = "Number of associated canonical events"
  ) 
```


```{r regions_over_time}
protests_by_region <- mpeds |> 
  mutate(region = case_when(
    !is.na(canada_metropolitan_area) ~ paste0(canada_region_name, " (CA)"),
    is.na(region) ~ NA_character_,
    TRUE ~ region)
    ) |> 
  mutate(date = floor_date(start_date, "month"),
         region = as_factor(region) |> 
           fct_reorder(ifelse(is.na(canada_metropolitan_area), 0, 1))) |> 
  group_by(region, date) |> 
  count() |> 
  drop_na() 

ggplot(protests_by_region,
       aes(fill = region, x = date, y = n)) + 
  geom_bar(stat = "identity") + 
  scale_x_date(breaks = "4 months",
               date_labels =  "%b",
               sec.axis = dup_axis(
                 breaks = seq(as.Date("2012-01-01"), as.Date("2018-01-01"), by = "year"),
                 labels = scales::label_date("%Y"),
               )
               ) + 
  scale_fill_brewer(type = "qual", palette = "Set1") + 
  labs(
    title = "Events over time by region",
    x = "Date (floored to months)",
    y = "Number of associated canonical events", 
    color = "U.S. Census region,\nor Canadian province"
  ) 
```


We can also begin to look at the top universities, counties, locations, or states
over time. This inevitably produces more complex summaries, and it can
be difficult to take an informative glimpse given so many categories, so I've only
shown the universities over time for now:

```{r unis_over_time}
uni_grouping <- tribble(
  ~university, ~group,
  "Columbia University in the City of New York", "Northeast",
  "Harvard University", "Northeast",
  "Tufts University", "Northeast",
  "Concordia University",  "Quebec",
  "McGill University", "Quebec",
  "University of Toronto", "Ontario",
  "Ryerson University", "Ontario",
  "University of California-Berkeley", "California",
  "University of California-Los Angeles", "California",
  "University of Chicago", "Midwest",
  "University of Michigan-Ann Arbor", "Midwest",
  "University of Wisconsin-Madison", "Midwest"
)

unis_over_time <- mpeds |> 
  mutate(date = floor_date(start_date, "month")) |> 
  group_by(university, date) |> 
  count() |> 
  drop_na() |> 
  filter(university %in% university_counts$university[1:10]) |> 
  mutate(university = str_wrap(university, 15)) |> 
  left_join(uni_grouping, by = "university")

uni_plot <- ggplot(unis_over_time, aes(x = date, y = n, fill = university)) + 
  geom_bar(stat = "identity") + 
  scale_fill_brewer(type = "qual", palette = "Paired") + 
  labs(
    x = paste0("Time (months)"),
    y = "Number of associated canonical events",
    title = "Top universities by protest count over time",
    caption = paste("Universities displayed are top 10 overall universities
    by protest count, not a top university in a given year.", 
    "Dates are aggregated to the month unit.")
  ) + 
  guides(fill = guide_legend(byrow = TRUE)) + 
  theme(legend.spacing.y = unit(0.2, "cm"))
uni_plot

uni_grouping_over_time <- mpeds |> 
  mutate(date = floor_date(start_date, "month")) |> 
  filter(university %in% university_counts$university[1:10]) |> 
  left_join(uni_grouping, by = "university") |> 
  group_by(group, date) |> 
  count() |> 
  drop_na() 

uni_grouped_plot <- ggplot(uni_grouping_over_time, aes(x = date, y = n, fill = group)) + 
  geom_bar(stat = "identity") + 
  scale_fill_brewer(type = "qual", palette = "Set1") + 
  labs(
    x = paste0("Time (floored to months)"),
    y = "Number of associated canonical events",
    title = "Top university groups by protest count over time",
    caption = paste("Dates are aggregated to the month unit.")
  ) 

uni_grouped_plot

```

```{r responses_over_time}
plot_response_over_time <- function(colname){
  unnested <- mpeds |>
    mutate("{{ colname }}" := map({{ colname }}, \(item){
      if(is.null(item)){
        return(NA)
      }
      return(item)
      })) |>
    select(start_date, {{colname}}) |> 
    unnest(cols = {{colname}}) |> 
    filter(!is.na({{colname}}), {{colname}} != "NA/Unclear")
  
  top_overall <- unnested |> 
    count({{colname}}) |> 
    arrange(desc(n)) |> 
    slice(1:9) |> 
    pull({{colname}}) 
  
  name_string <- rlang::as_name(enquo(colname)) |> 
    str_replace_all("_", " ") |> 
    str_to_sentence() 
  
  unnested |> 
    filter({{colname}} %in% top_overall) |> 
    mutate(date = floor_date(start_date, "month")) |> 
    group_by({{colname}}, date) |> 
    count() |> 
    ggplot(aes(x = date, y = n, fill = {{colname}})) + 
    geom_bar(stat = "identity") + 
    scale_fill_brewer(type = "qual", palette = "Paired") + 
    labs(
      title = paste(name_string, "over time"),
      y = "Number of occurrences",
      x = "Date (month)",
      fill = name_string
    ) +
    guides(fill = guide_legend(byrow = TRUE)) +
    theme(
      legend.spacing.y = unit(0.2, "cm")
    )
}

plot_response_over_time(police_presence_and_size)
plot_response_over_time(police_activities)
plot_response_over_time(type_of_police)
plot_response_over_time(university_action_on_issue)
plot_response_over_time(university_discourse_on_issue)
plot_response_over_time(university_reactions_to_protest)
```

```{r issues_over_time}
# Tad tricky since there are often multiple issues per role
issues <- mpeds |> 
  select(issue, start_date) |> 
  unnest(cols = issue) |> 
  mutate(date = floor_date(start_date, "month"),
         issue = str_wrap(issue, 20),
         ) |> 
  filter(issue != "_Not relevant")

racial_issue <- mpeds |> 
  select(issue = racial_issue, start_date) |> 
  unnest(cols = issue) |> 
  mutate(date = floor_date(start_date, "month"),
         issue = str_wrap(issue, 20),
         ) |> 
  filter(issue != "_Not relevant")

# 42 rows is still readable so I'll just print the entire table
issues_count <- issues |> 
  count(issue) |> 
  arrange(desc(n)) 
issues_count |>
  mutate(issue = str_replace_all(issue, "\n", ' ')) |> 
  kable()

racial_issue_count <- racial_issue |> 
  count(racial_issue = issue) |> 
  arrange(desc(n))

racial_issue_count |> 
  mutate(racial_issue = str_replace_all(racial_issue, "\n", ' ')) |> 
  kable()

top_racial_issues <- racial_issue |> 
  filter(issue %in% racial_issue_count$racial_issue[1:4]) |> 
  mutate(issue = paste(issue, "(racial)"))


# 42 lines on a plot on the other hand is like 35 too many
issues |> 
  filter(issue %in% issues_count$issue[1:7]) |> 
  group_by(issue, date) |> 
  count() |> 
  drop_na() |> 
  ggplot(aes(x = date, y = n, fill = issue)) + 
  geom_bar(stat = "identity") + 
  scale_fill_brewer(type = "qual", palette = "Set1") + 
  labs(
    title = "Issue breakdown over time",
    y = "Number of associated canonical events",
    x = "Date (month)",
    caption = "Excludes racial issues."
  ) +
  guides(fill = guide_legend(byrow = TRUE)) +
  theme(
    legend.spacing.y = unit(0.5, "cm")
  )
  
issues |> 
  filter(issue %in% issues_count$issue[1:4]) |> 
  bind_rows(top_racial_issues) |> 
  mutate(issue = as.factor(issue) |> 
           fct_relevel(c(
             issues_count$issue[1:4],
             racial_issue_count$racial_issue[1:4])
             )) |> 
  group_by(issue, date) |> 
  count() |> 
  drop_na() |> 
  ggplot(aes(x = date, y = n, fill = issue)) + 
  geom_bar(stat = "identity") + 
  scale_fill_brewer(type = "qual", palette = "Set1") + 
  labs(
    title = "Issue breakdown over time",
    y = "Number of associated canonical events",
    x = "Date (month)",
    caption = "Includes racial issues."
  ) +
  guides(fill = guide_legend(byrow = TRUE)) +
  theme(
    legend.spacing.y = unit(0.5, "cm")
  )
  

# one where y-axis percentage of total issues in given month
# We want to divide the number of CEs associated with an issue in a month
# by the total number of CEs in a month; that latter statistic we have to get by 
# using `mutate` instead of `summarize` in order to preserve the tibble format, 
# but that means that the resulting column of counts will be duplicated,
# as in we'll have one count of total issues for each issue occurrence in a month
# I use `n_events1]` to resolve that
issues |> 
  group_by(date) |> 
  mutate(
    n_events = n()
  ) |> 
  group_by(issue, date)  |> 
  summarize(
    issue_pct = n() / n_events[1],
    .groups = "drop"
  ) |> 
  filter(issue %in% issues_count$issue[1:7]) |> 
  ggplot(aes(x = date, y = issue_pct, color = issue)) + 
  geom_line()
```


# Basic summary plots by variable

```{r summary}
mpeds |> 
  select(where(function(x){is.numeric(x) || is.logical(x)}),
                 -canonical_id, -starts_with("location"),
                 -year, -uni_id, -size_category, -link) |> 
  pivot_longer(cols = everything()) |> 
  filter(name != "NA") |> 
  group_by(name) |> 
  summarize(
    type = ifelse(is.numeric(pull(mpeds[name[1]], 1)), "numeric", "boolean"),
    mean = mean(value, na.rm = TRUE),
    sd = ifelse(type == "boolean", NA_integer_, sd(value, na.rm = TRUE))
  ) |> 
  mutate(across(where(is.numeric), ~round(., 3))) |> 
  arrange(type) |> 
  kable()
```

For boolean variables, "mean" is the proportion that they are TRUE. Many of the
variables recorded in MPEDS allowed for the input of multiple values, 
so those are handled as list-cols and not shown here.

```{r pairs, warning = FALSE, width = 8, height = 8}
mpeds |> 
  select(where(is.numeric), -canonical_id, -starts_with("location"),
                 -year, -uni_id, -size_category, -enrollment_count) |> 
  ggpairs(progress = FALSE, lower = list(
    continuous = wrap(ggally_points, size = 0.1, alpha = 0.2)
    ),
    title = "Glimpse at relationships among numeric variables"
    )

```

The pairs plot is still very difficult to read after adjustments. This should be
treated as a glimpse or overview, and more detailed and cleaner plots will be made
later on.

```{r distributions}
mpeds |> 
  select(where(is.numeric), -canonical_id, -starts_with("location"),
                 -year, -uni_id, -size_category) |> 
  pivot_longer(everything()) |> 
  drop_na() |> 
  ggplot(aes(value)) + 
  geom_histogram() + 
  facet_wrap(~name, scales = "free") + 
  labs(
    title = "Glimpse of numeric covariates at the canonical event level",
    y = "Number of occurrences",
    x = "Value of variable",
  )

```

# Trying out joins with protest data

To recap from our last conversation, it's a bit difficult to join the CCC data
and our data since a lot of MPEDS data points could presumably be in the CCC
records. Then CCC data could be telling us that there was a protest in the same
county, when it could just be talking about the same protest in MPEDS and
essentially be turning data quality into another covariate. 

We discussed two solutions to this problem to avoid deduplication:

- Join so that CCC protests occurring one, three, five, or seven days before the
  MPEDS protest date are matched; the CCC variable then conceptually becomes 
  "was there a recent protest in the same county." Thus protests won't find
  a match only because of duplicates
- Join only after filtering the CCC dataset so that rows with keywords related
  to universities are kicked out -- things like teachers, faculty, students,
  colleges, universities. This is less ideal than the above strategy because it
  is so nonspecific, potentially missing many university matches and kicking out
  protests related to primary and secondary schools.
  
The following chunk gives a glimpse at total number of matches:

```{r external_protest_sources}
ccc <- tar_read(ccc) |> 
  distinct() |> 
  rename(protest_date = ccc_protest_date)

blm <- tar_read(elephrame_blm) |> 
  distinct() |> 
  rename(protest_date = blm_protest_date)
  
# We want to assess successful match rates in a sensible manner, which means 
# restricting the MPEDS dataset to just the protest years available in either 
# CCC or Elephrame
n_ccc <- mpeds |> 
  filter(start_date > min(ccc$protest_date, na.rm = TRUE)) |> 
  nrow()
n_elephrame <- mpeds |> 
  filter(start_date > min(blm$protest_date, na.rm = TRUE)) |> 
  nrow()

test_date_diffs <- function(protests){
  # a version where dates are a list-col, to assist in the testing below
  protest_dates <- protests |> 
    group_by(fips) |> 
    summarize(recent_protest_dates_lst = list(protest_date))
  matched_mpeds <- mpeds |> 
    left_join(protest_dates, by = "fips") 
  
  # return a TRUE value if any protests in `vec` occurred between 
  # a given date and `diff` days after that date
  compute_protests <- function(vec, date, diff){
    if(is.na(date)) return(NA)
    any(vec %in% (date + 1):(diff + date))
  }
  
  match_date_diff <- function(diff){
    # For each canonical event, use the `recent_protest_dates_lst` column
    # representing protests in the same county to assess if any
    # those nearby protests happened recently
    recent_protests <- matched_mpeds |> 
      mutate(
        has_recent_protest_nearby = unlist(map2(
          recent_protest_dates_lst, start_date,
          function(x,y){compute_protests(x, y, diff)}
        ))
      )
    
    n_matches <- sum(recent_protests$has_recent_protest_nearby, na.rm = TRUE)
    
    tribble(~date_offset, ~recent_protests,
            diff, n_matches, 
            )
    
  }
  return(map_dfr(c(0,1,3,5,7), match_date_diff))
}

match_results <- map_dfr(list("CCC" = ccc, "Elephrame" = blm),
                         test_date_diffs, .id = "source") |> 
  mutate(match_percentage = ifelse(
    source == "CCC", 100 * recent_protests / n_ccc,
    100 * recent_protests / n_elephrame
  ))

kable(match_results)

```

Here, the `match_percentage` column indicates how many canonical events saw another
protest occur in the same county within `diff` days, according to the dataset
in `source`. The fact that the match rate for 0 is much higher than 1 for both
Elephrame and CCC indicates that there is some double-counting of protests;
rather than multiple protests occurring concurrently, we may have recorded a protest
in our dataset that is also present in another dataset.

So it seems that there are a fair number of duplicates occurring if we don't
have a date offset, but once we add one (of any days) that pretty much solves
the data quality issue.

That being said, the likely larger problem with the CCC data is that it's only 
available after 2017, so it may not be relevant even after we become satisfied
with the deduped match process. This can be refined a little bit by adding in 
Elephrame data on BLM protests, but we've had problems there already, and the topic 
differences mean we can't pretend we have complete data.

# Maps and related things

```{r maps, message = FALSE, warning = FALSE}
county_sf <- counties(keep_zipped_shapefile = TRUE, progress_bar = FALSE) |> 
  select(fips = GEOID)
us_sf <- states(progress_bar = FALSE) |> 
  filter(!(NAME %in% c("Hawaii", "Puerto Rico", "American Samoa", 
                       "United States Virgin Islands", 
                       "Commonwealth of the Northern Mariana Islands",
                       "Alaska", "Guam"))) |> 
  st_union()
```

```{r mpeds_map}

mpeds_sf |> 
  st_transform(st_crs(county_sf)) |>
  mutate(geometry = st_jitter(geometry, factor = 0.005)) |> 
  ggplot() + 
  geom_sf(data = us_sf, fill = "white", color = "gray") + 
  geom_sf(size = 0.1, alpha = 0.2) + 
  lims(
    x = c(-130, -60),
    y = c(20, 55)
  ) +
  labs(
    title = "Spread of canonical events and geocoded locations",
    subtitle = "Locations jittered slightly, by 0.005*bounding box diagonal.",
    caption = "Alaska, Hawaii, a few other locations with only a\nfew protests excluded in this map only."
  ) + 
  theme_void() + 
  theme(text = element_text(family = "Lato"),
        plot.title = element_text(size = 20))
```

# Investigating specific movements: Mizzou and Quebec solidarity protests

The Mizzou umbrella was linked 104 times, with 98 unique canonical events inside.

```{r mizzou, warning = FALSE}
tar_load(canonical_event_relationship)

mizzou_umbrella_id <- mpeds |>
  filter(key == "Umbrella_Mizzou_Anti-Racism_2015_Oct-Nov") |> 
  pull(canonical_id)

mizzou_events <- canonical_event_relationship |> 
  filter(canonical_id2 == mizzou_umbrella_id) |> 
  group_by(canonical_id1) |> 
  slice_head(n = 1) |> 
  left_join(mpeds_sf, by = c("canonical_id1" = "canonical_id"))  |> 
  st_as_sf()

mizzou_events |> 
  mutate(geometry = st_jitter(geometry, factor = 0.005)) |> 
  ggplot() +
  geom_sf(data = us_sf) +
  geom_sf(aes(color = relationship_type), alpha = 0.5, size = 1) + 
  labs(
    title = "Spread of events within the Mizzou umbrella",
    color = "Relationship type",
    caption = "Jittered slightly to illustrate density"
  ) + 
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
  )

mizzou_events |> 
  mutate(date = start_date) |> 
  group_by(date) |> 
  count() |> 
  ggplot(aes(x = date, y = n)) + 
  geom_line() + 
  geom_vline(xintercept = as.Date("2015-08-26"), color = "red") +
  annotate(geom = "text", label = "Mizzou grad student\nstrike start date",
           color = "red", x = as.Date("2015-09-12"), y = 10) + 
  scale_x_date(date_labels = "%b %y", date_breaks = "1 month") + 
  labs(
    x = "Date",
    y = "Number of canonical events starting on date",
    title = "Mizzou solidarity protest occurrences over time"
  )
  

```
